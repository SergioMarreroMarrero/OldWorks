Cross-Modal Learning to Rank via Latent Joint Representation,

# Cross-modal ranking is a research topic that is imperative to many applications involving multimodal data. Discovering a joint representation for multimodal data and learning a ranking function are essential in order to boost the cross-media retrieval (i.e., image-query-text or text-query-image). In this paper, we propose an approach to discover the latent joint representation of pairs of multimodal data (e.g., pairs of an image query and a text document) via a conditional random field and structural learning in a listwise ranking manner. We call this approach cross-modal learning to rank via latent joint representation (CML2R). In CML2R, the correlations between multimodal data are captured in terms of their sharing hidden variables (e.g., topics), and a hidden-topic-driven discriminative ranking function is learned in a listwise ranking manner. The experiments show that the proposed approach achieves a good performance in cross-media retrieval and meanwhile has the capability to learn the discriminative representation of multimodal data.

{data structures;information retrieval;random processes;CML2R;conditional random field;cross-media retrieval;cross-modal learning;cross-modal ranking;discriminative multimodal data representation;hidden-topic-driven discriminative ranking function;latent joint representation;listwise ranking manner;structural learning;Approximation methods;Correlation;Joints;Loss measurement;Manganese;Vectors;Cross-modal Ranking;Cross-modal ranking;Latent Joint Representation;Learning to Rank;latent joint representation;learning to rank},



