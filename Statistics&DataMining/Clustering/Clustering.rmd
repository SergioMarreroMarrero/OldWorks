---
title: "Algoritmos de clustering y búsqueda de asociaciones"
author: "UOC - Master BI - Business Analytics (Sabina Hernández)"
date: "Junio del 2017"
output:
  pdf_document:
    toc: yes
  html_document:
    fig_height: 5
    fig_width: 7
    number_sections: yes
    theme: journal
    toc: yes
    toc_depth: 2
  word_document: default
---


******
# Introducción
******

Esta práctica está basada en los puntos 3.3.2, 3.3.3 y 3.4 del material didáctico (Business Analytics) de la asignatura. En los puntos 3.3.2 y 3.3.3 se explican procedimientos de segmentación no jerárquica para la formación de grupos que, respecto a la información utilizada, sean homogéneos dentro de si mismos y heterogéneos entre unos y otros. El punto 3.4 se centra en la búsqueda de asociaciones las cuales en nuestro caso se darán entre las características de los asegurados que forman parte de la catera analizada.   

A lo largo de la práctica se proponen una serie de representaciones gráficas que ayudan a la interpretación de los resultados, sin embargo, podéis insertar más visualizaciones de las propuestas o incluso más código del estrictamente exigido en los ejercicios, eso sí, siempre con el objetivo de completar y mejorar el estudio propuesto.  

En esta práctica importaremos los datos desde un fichero de texto .csv con los campos delimitados por ";". Dichos datos corresponden a la información sobre algunas características de una muestra de asegurados procedentes de una cartera de seguros de automóvil. Los datos han sido extraídos de una cartera de asegurados real, aunque para garantizar la confidencialidad de la información se ha seleccionado una muestra no representativa o sesgada de la realidad. 

******
# Objetivos e información disponible
******

Los objetivos de esta tercera PEC son dos. El primero se centra en la determinación de distintos perfiles de asegurados del automóvil. El segundo objetivo es encontrar asociaciones entre las características del asegurado y el tipo de garantía contratada. 

Las variables que se definen en la base de datos y sus contenidos son:

--poliza: Identificador de póliza

--Sexo: Sexo del cliente

--sri: Situación de riesgo o zona de circulación urbana o no urbana

--gdi: Contratada garantía de daños propios o no

--sin: Número de siniestros en el año analizado

--ant_comp: Antigüedad del cliente en la compañía (en años)

--ant_perm: Antigüedad del permiso de conducir del asegurado (en años)

--edad: Edad del asegurado (en años)

--ant_veh: Antigüedad del vehículo asegurado (en años).


******
# Apartados de la práctica
******
El código R que utilizaremos en la práctica se divide en apartados según las tareas que iremos realizando:  

* Directorio de trabajo

* Importación del fichero de datos .csv. Manipulación y representación de las variables

* Agrupación no jerárquica: Algoritmo kmeans

  * Normalización de atributos
  
  * Uso de la función kmeans() para la formación de cluster (o perfiles de individuos)
  
  * Elección del número de clústers
  
* Búsqueda de asociaciones

  * Resultados por defecto del algoritmo apriori
  
  * Resultados fijando el soporte y la confianza
  
  * Resultados fijando las consecuencias (rhs)

* Ejercicios PEC3: Análisis cluster con kmeans

* Ejercicios PEC3: Búsqueda de asociaciones  

******
# Directorio de trabajo
******
Antes de pasar a la importación y análisis de los datos definimos un directorio de trabajo o carpeta donde tenéis guardado el fichero de datos. Recordad que si abrís el RStudio desde vuestro directorio de trabajo, pulsando sobre el fichero .RMD que se os proporciona como parte del enunciado, este paso no haría falta.
```{r,eval=TRUE,echo=TRUE}
#setwd("Pon aquí el directorio utilizado")
#Cambiar el argumento de setwd() con vuestro directorio, recordad utilizad las barras /.
```

******
# Importación del fichero de datos .csv. Manipulación y representación de las variables.
******
En primer lugar leemos el fichero de datos con extensión .csv que contiene la información de las 18.008 pólizas analizadas y mostramos su cabecera.
```{r,eval=TRUE,echo=TRUE}
# Lectura de datos
Cartera<-read.table("Datos_analisis_cluster.csv",head=TRUE,sep=";")
head(Cartera)
```
A continuación describimos su contenido con la función summary() y con algunos gráficos. Observamos que para las variables cuantitativas la función summary() proporciona una serie de estadísticos descriptivos relacionados con la posición de la variable (media, mediana, máximo, mínimo,...). Sin embargo, para las variables cualitativas el resultado muestra las frecuencias absolutas (número de casos) de las categorías de las variables.
```{r,eval=TRUE,echo=TRUE}
summary(Cartera)
```

Realizamos algunas representaciones gráficas para describir la base de datos Cartera, utilizamos las herramientas gráficas adecuadas para cada tipo de variable: Cualitativa o Cuantitativa. Recordad que, antes de realizar cualquier análisis, es imprescindible estudiar el comportamiento univariante y bivariante de las variables.
```{r,eval=TRUE,echo=TRUE}
plot(Cartera[c("ant_comp","ant_perm")], xlab="Fidelidad", ylab="Experiencia") 
title(main="Nube de puntos original", col.main="blue", font.main=1)

freq<-table(Cartera$sin)
freq
barplot(freq,xlab="Número de siniestros", ylab="Frecuencia")
title(main="Número de siniestros", col.main="blue", font.main=1)

table(Cartera$Sexo,Cartera$sin)
prop.table(table(Cartera$Sexo,Cartera$sin))
barplot(prop.table(table(Cartera$Sexo,Cartera$sin)),col=c("darkblue","red"))
legend(5,0.8,c("Hombre","Mujer"),fill = c("darkblue","red"))
```

******
# Agrupación no jerárquica: Algoritmo kmeans
******
******
## Normalización de atributos
******
El objetivo es utilizar la información cuantitativa relacionada con la experiencia (edad y ant_perm), con la fidelidad (ant_comp), con el vehículo (ant_veh) y con la siniestralidad (sin) para segmentar a los asegurados. Para ello, en primer lugar, definimos la base de datos con las variables cuantitativas que utilizamos en la segmentación, el resto de variables pueden servir para caracterizar los grupos formados o para otro tipo de análisis que plantearemos más adelante.
```{r,eval=TRUE,echo=TRUE}
clus<-Cartera[,c("sin","ant_comp","ant_perm","edad","ant_veh")]
```

La varianza de las variables (o su rango de valores) utilizadas en el análisis son distintas debido a que miden características diferentes de los individuos y de su vehículo. Por ejemplo, entre las variables utilizadas en el cluster hay algunas que miden el número de años y otra que mide el número de siniestro, es decir, las escalas son muy distintas. Por tanto, antes de iniciar el proceso de segmentación es necesario normalizar los valores de las variables para eliminar el efecto de las distintas escalas de medida, esto equivale a restarles su media y dividirlas por su desviación estándar.

Para la normalización de las variables en la base de datos clus, en primer lugar copiamos su contenido en clus2:
```{r,eval=TRUE,echo=TRUE}
clus2<-clus
```

Remplazamos las columnas de clus2 por las columnas de clus normalizadas:
```{r,eval=TRUE,echo=TRUE}
 clus2[,c("sin")] <- (clus$sin-mean(clus$sin))/sd(clus$sin)
 clus2[,c("ant_comp")] <- (clus$ant_comp-mean(clus$ant_comp))/sd(clus$ant_comp)
 clus2[,c("ant_perm")] <- (clus$ant_perm-mean(clus$ant_perm))/sd(clus$ant_perm)
 clus2[,c("edad")] <- (clus$edad-mean(clus$edad))/sd(clus$edad)
 clus2[,c("ant_veh")] <- (clus$ant_veh-mean(clus$ant_veh))/sd(clus$ant_veh)
```

Realizamos algunas representaciones gráficas para describir las variables normalizadas y comprobamos que la nube de puntos representada es igual a la original, lo único que cambia es la escala de los ejes.
```{r,eval=TRUE,echo=TRUE}
#Normalizadas
plot(clus2[c("ant_comp","ant_perm")], xlab="Fidelidad", ylab="Experiencia") 
title(main="Nube de puntos normalizados", col.main="blue", font.main=1)

#Originales
plot(clus[c("ant_comp","ant_perm")], xlab="Fidelidad", ylab="Experiencia") 
title(main="Nube de puntos originales", col.main="blue", font.main=1)
```

A PARTIR DE AHORA TRABAJAMOS CON LOS DATOS NORMALIZADOS.


******
## Uso de la función kmeans() para la formación de cluster (grupos o perfiles de individuos) 
******
Los algoritmos de segmentación no supervisados, como es el kmeans(), requieren que el analista determine cuál es el número de clústers (grupos) a formar, de hecho, la función kmeans() incorpora como parámetro el número de clústers (centers=).

Para seleccionar el número de grupos podemos utilizar criterios subjetivos o criterios objetivos. Los criterios subjetivos se basan en la visualización de los resultados para determinar el número de clústers más apropiado o en la simple experiencia. A continuación, utilizamos la función kmeans() para formar 3 grupos de individuos y visualizamos algunos resultados como son: los centros de grupos (centers), la suma de cuadrados totales (totss), las sumas de cuadrados dentro de cada grupo y para todos de forma conjunta (withinss y tot.withinss) y la suma de cuadrados entre grupos (betweenss). 



```{r,eval=TRUE,echo=TRUE}
set.seed(123)
clus2_k3<-kmeans(clus2,centers=3)
clus2_k3$centers
clus2_k3$totss
clus2_k3$withinss
clus2_k3$tot.withinss
clus2_k3$betweenss
```

******
## Elección del número de clústers   
******
Para la selección del número de clústers también existen criterios objetivos los cuales están basados en la optimización de un criterio de ajuste.

Los criterios de ajustes en el kmeans() se basan en los conceptos de sumas de cuadrados entre grupos (betweens) y dentro de grupos (withins). Hay que tener en cuenta que la suma de cuadrados entre grupos (betweenss) más las sumas de cuadrados dentro de grupos (tot.withinss) nos proporciona la suma de cuadrados totales (tots). Recordad también que las sumas de cuadrados corresponden a los numeradores de las varianzas correspondientes. 

Una segmentación 'óptima' es aquella donde los individuos pertenecientes a un mismo grupo son lo más homogéneos posible y los individuos pertenecientes a distintos grupos son lo más heterogéneos posible. Dicha segmentación coincidirá con aquella que, teniendo un número de grupos razonable, posee una suma de cuadrados entre suficientemente grande y, por tanto, una suma de cuadrados dentro lo suficientemente pequeña. Es decir, la varianza dentro de grupos debe ser reducida (individuos dentro de un mismo grupo tiene que ser similares) y la varianza entre grupos debe ser grande (individuos de distintos grupos tienen que ser distintos). También, tenemos que tener en cuenta que a medida que el número de grupos aumenta la suma de cuadrados entre aumenta y, por tanto, la suma de cuadrados dentro disminuye, por tanto, el analista a de decidir cuando el aumento de la suma de cuadrados entre o, alternativamente, la disminución de la suma de cuadrados dentro no son lo suficientemente pronunciados. Por ejemplo, comparamos los resultados para los casos de formar 2 y 3 grupos.

```{r,eval=TRUE,echo=TRUE}
#Suma de cuadrados entre grupos
kmeans(clus2,2)$betweenss
kmeans(clus2,3)$betweenss

#Suma de cuadrados dentro grupos
kmeans(clus2,2)$tot.withinss 
kmeans(clus2,3)$tot.withinss

#Suma de cuadrados total
kmeans(clus2,2)$totss 
kmeans(clus2,3)$totss
```

A continuación, definimos el modo de obtener un gráfico que nos represente la suma de cuadrados entre grupos en función del número de grupos.
```{r,eval=TRUE,echo=TRUE}
set.seed(123)
bss <- kmeans(clus2,centers=1)$betweenss
 for (i in 2:10) bss[i] <- kmeans(clus2,centers=i)$betweenss

plot(1:10, bss, type="l", xlab="Número de grupos",ylab="Sumas de cuadrados entre grupos")
```

******
# Búsqueda de asociaciones
******
En este apartado de la práctica nos centraremos en la búsqueda de asociaciones entre algunos atributos de los individuos analizados, los cuales están registrados en la base de datos Cartera definida al inicio de esta PEC. En concreto trabajaremos con las variables: Sexo, sri y gdi. Denominamos a nuestra nueva base de datos aso_car.

```{r,eval=TRUE,echo=TRUE}
aso_car<-Cartera[,c("Sexo","sri","gdi")]
head(aso_car)
```

Para realizar el análisis necesitamos instalar el paquete "arules". Para ello marca la siguiente fila y ejecuta de forma aislada:

install.packages("arules")


******
## Resultados por defecto del algoritmo apriori
******
El algoritmo apriori fue diseñado para la búsqueda de asociaciones entre los productos que forman parte de la cesta de la compra en el supermercado, hipermercado o gran superficie. El objetivo era determinar que productos eran causa de la compra de otros. Sin embargo, el algoritmo apriori puede generalizarse para la búsqueda de asociaciones entre cualquier conjunto de items. En esta PEC el objetivo será buscar asociaciones entre los items que caracterizan a los asegurados, en total 6 características: hombre, mujer, urbano, no urbano, garantía daños propios contratada y no contratada. Para ello se definen las denominadas "reglas de asociación". Una "regla de asociación" está formada por uno o más antecedentes y una consecuencia y, en su forma más simple, está caracterizada por su "soporte" (porcentaje de casos en los que se dan los antecedentes conjuntamente) y su confianza (porcentaje de casos en los que se da la consecuencia junto con los antecedentes respecto a las veces que se dan los antecedentes conjuntamente). 

A continuación, se realiza la búsqueda de reglas de asociación que el algoritmo apriori determina por defecto. Observamos que únicamente encuentra una regla de asociación que nos indica que aquellos asegurados hombres con situación de riesgo urbana (antecedentes) no contratan la garantía daños propios (consecuencia), el soporte de esta regla es del 21,21% y su confianza es del 80,88%. El parámetro lift o apalancamiento (=1.03) es el cociente entre la confianza de la regla y el soporte de la consecuencia, este parámetro permite valorar si dicha consecuencia tiene más probabilidad cuando se da el antecedente o en general.

```{r,eval=TRUE,echo=TRUE}
library(arules)
apriori(aso_car)
inspect(apriori(aso_car))
```

******
## Resultados fijando el soporte y la confianza
******
De los resultados del apartado 7.1 también puede deducirse que, por defecto, el algoritmo apriori de R busca reglas de asociación con un soporte del 10% o superior y una confianza del 80% o superior. Sin embargo, dada la frecuencia de casos que tenemos en nuestra base de datos y su elevada dispersión, se decide reducir el soporte al 5% y la confianza al 70%. Se observa que con estos parámetros el número de reglas asciende a 18 y las 2 primeras no tienen antecedente (en estos 2 casos el soporte coincide con la confianza).

```{r,eval=TRUE,echo=TRUE}
apriori(aso_car,parameter = list ( supp = 0.05 , conf= 0.7 , target = "rules"))

inspect(apriori(aso_car,parameter = list ( supp = 0.05 , conf= 0.7 , target = "rules")))
```

******
## Resultados fijando las consecuencias (rhs):
******
También, es importante tener en cuenta que en ocasiones el interés radica en la búsqueda de antecedentes dada una consecuencia. En el ejemplo de esta PEC el interés podría ser analizar los antecedentes asociados a las garantías contratadas (en este caso representadas por las dos categorías de la variable gdi). Para ello se le indica al algoritmo que busque las reglas de asociación con dichos antecedentes. 

```{r,eval=TRUE,echo=TRUE}
apriori(aso_car,parameter = list ( supp = 0.05 , conf= 0.7 , target = "rules"),appearance = list(rhs=c("gdi=No contratada", "gdi=Garantia danos propios contratada"), default="lhs"))
inspect(apriori(aso_car,parameter = list ( supp = 0.05 , conf= 0.7 , target = "rules"),appearance = list(rhs=c("gdi=No contratada", "gdi=Garantia danos propios contratada"), default="lhs")))
```

******
# Ejercicios PEC3
******

******
## Ejercicios PEC3: Análisis cluster con kmeans
******

### Ejercicio 1
En el apartado 6.1, se seleccionan las variables cuantitativas para realizar la segmentación. ¿Sería posible incluir variables categóricas? ¿Cómo se incluirían? Elige una variable categórica e inclúyela en una segmentación K-means.

###Respuesta 1

El algoritmo k-mean esta diseñado para trabajar con datos numéricos, los cuales son susceptibles de una métrica que permita comparar las distancias. Transformar un espacio categórico a un espacio numérico puede ser una tarea sencilla, pero si lo que se quiere hacer es asociar alguna medida con la que poder comparar para tomar decisiones acerca de los segmentos óptimos, resulta más complejo. Por otro lado, se podría utilizar alguna medida que permita comparar datos categóricos o bien aumentar la dimensionalidad del problema trasladando los datos categóricos a una notación numérica que no beneficie a ninguna categoría, como por ejemplo el sistema "one-hot".

```{r,eval=TRUE,echo=TRUE}
# Código respuesta 1

```

### Ejercicio 2
En el apartado 6.2 se ha establecido una semilla mediante el comando set.seed(). ¿Por que motivo hay que incluir una semilla? ¿Qué pasaría si no se incluye la semilla? Justifique su respuesta y compleméntela con ejemplos.

###Respuesta 2
El algoritmo k-mean tiene cieta componente estocástica, y esto es debido a que al inicio los centroides son escogidos aleatoriamente. Cuando fijamos una semilla obligamos a que siempre se usen los mismos números aleatorios. Esto nos permite poder comparar distintas versiones del mismo algoritmo eliminando toda posibilidad de que los cambios se deban al fenómeno aleatorio. A continuación vamos a realizar un pequeño ejemplo en el que cambio la semilla y vemos como el resultado no es exactamente el mismo. En el ejemplo se prueban cuatro semillas diferentes y se observa que se llegan a resultados similares. Si el espacio de optimización tiene mínimos locales entonces se podrían llegar a obtener resutlados completamente diferentes, dependiendo del centroide inicial. En este caso se observa que los resultados a los que se llegan son idénticos (salvo por algún decimal, justificándose este ligeron cambio en un criterio de parada ligeramente prematuro). 



```{r,eval=TRUE,echo=TRUE}
seeds = c(123,321, 12, 14)
for (seed in seeds){
  
  print(paste("Seed fixed in ",seed, sep = " "))
  
  set.seed(seed)
  clus2_k3<-kmeans(clus2,centers=3)
  print(clus2_k3$centers)
  
  #print(clus2_k3$totss)
  #print(clus2_k3$withinss)
  #print(clus2_k3$tot.withinss)
  #print(clus2_k3$betweenss)
}
```


### Ejercicio 3
En el apartado 5 se muestra la nube de puntos original de la relación entre la Experiencia y la Fidelidad. En este ejercicio se pide colorear los puntos en función al segmento al que se clasifica, incluyendo el atributo col=clus2_k3$cluster, dentro de la función plot. Muestre el nuevo gráfico e interprete los resultados. ¿Hay diferencias en la Experiencia y la Fidelidad de los tres segmentos?

###Respuesta 3
Por imposición, el algoritmo detecta tres grupos. Hay claras diferencias en los tres grupos respecto de los ejes en los que se ha representado. Hay que recordar que cada grupo representan sujetos con altas similitudes en el espacio de todas las características(numéricas) y que lo que observamos es una proyeccion en ejes que no representan todos los campos que se tuvieron en cuenta para calcular esas distancias.El grupo que se mantiene vertical a la izquierda , responde al grupo de "clientes relativamente nuevos". El grupo que se montiene horizontale y abajo representa clientes con "corta experiencia". El grupo que aparace más hacia las esquina superior derecha, recoge a un tipo de cliente más común, ya que se ve incluse que invade el espacio de los otros dos.

```{r,eval=TRUE,echo=TRUE}
plot(Cartera[c("ant_comp","ant_perm")], xlab="Fidelidad", ylab="Experiencia", col=clus2_k3$cluster) 
title(main="Nube de puntos original", col.main="blue", font.main=1)
```

### Ejercicio 4

Una de las funciones más comunes de las segmentaciones es separar conjuntos de clientes con características comunes para gestionarlos de forma específica.

En el apartado 6.2 se muestran los centroides de los 3 segmentos, pero la información no es fácilmente interpretable puesto que las variables están normalizadas. Para poder interpretar las variables deberíamos invertir la normalización. Así, para la semgnetación kmeans con k=3 tenemos que los centros originales serían:

```{r,eval=TRUE,echo=TRUE}
aggregate(clus, by = list(clus2_k3$cluster), mean)
```
En este caso, podríamos ver diferencias entre los segmentos, por ejemplo, la edad media del primer segmento es 26,7 años, mientras que los segmentos 2 y 3 están conformados por personas con edad media cercana a los 48 años. Por lo que las estrategias de gestión serán distintas.

En este apartado se pide, realizar una segmentación para k=5 utilizando la información normalizada en el apartado 6.1 (data.frame clus2) e interprete los 5 grupos que aparecen. Para unificar los resultados fijar la semilla en 123 (set.seed(123))

###Respuesta 4
Destacamos un tipo de cliente (Grupo 1) que antes no era tan visible, y que gracias al aumento de de los grupos aparece. Este perfil de cliente tiene la caractarística de tener un promedio de accidentes bastante más alto que el resto. Por otro lado, aparece una categoría de clientes (Grupo 2,3,5) que tienen entorno a las 45-50 años y cuya principal diferencia es la antiguedad del vehículo. Finalmente aparece un grupo de clientes (Grupo 4) (que también apareció con k = 3) cuya principal caracerísticas es que su edad media es de 26 años.

```{r,eval=TRUE,echo=TRUE}
  set.seed(123)
  clus2_k5<-kmeans(clus2,centers=5)
  print(clus2_k3$centers)
  aggregate(clus, by = list(clus2_k5$cluster), mean)
  
```

###Ejercicio 5

En el apartado 6.3 sobre la elección del número de clústers se muestra un gráfico que toma como variable referencia la suma de cuadrados entre '$betweenss'.

Dibuja un gráfico equivalente, tomando como referencia la suma de cuadrados en '$tot.withinss' e interpreta el resultado proponiendo un número de clústers adecuado para el juego de datos.

###Respuesta 5
Anteriormente se representó las distancias de los grupos a medida que se aumentaban las clases. Cuanto más distantes estos grupos, mas separados estarán estos grupos. Ahora se esta representando las distancias de las muestras de cada grupo respecto de su centroide. En esta situación lo que interesa es minimizar, es decir, que entre ellas estén lo menos distante posible. Sin embargo, lo natural es que a medida que hayan más grupos, menor sean estas distancias, con lo cual, hay que tomar una decisión basada en cuando ya no es necesario seguir aumentando el número de grupos. Observando dicho gráfico, se infiere que a partir de 5 grupos, la mejoría obtenida por aumentar un grupo más, no es lo suficientemente pronunciada en comparación con las experimentadas anteriormente. Con lo cual, el número de cluester propuesto es de 5.
```{r,eval=TRUE,echo=TRUE}
set.seed(123)
bss <- kmeans(clus2,centers=1)$tot.withinss
 for (i in 2:10) bss[i] <- kmeans(clus2,centers=i)$tot.withinss

plot(1:10, bss, type="l", xlab="Número de grupos",ylab="Sumas de cuadrados entre grupos")
```

******
##Ejercicios PEC3: Búsqueda de asociaciones  
******

###Ejercicio 6
¿Tiene sentido normalizar variables para un algoritmo de búsqueda de asociaciones como Apriori?

###Respuesta 6
Debido a que no se está tomando ninguna medida para luego comparar en base a regla, la respuesta es no, no tiene sentido. 

```{r,eval=TRUE,echo=TRUE}
# Código respuesta 6
```

###Ejercicio 7

En el apartado 7 se han seleccionado las variables categóricas para la búsqueda de asociaciones. Analice y justifique el motivo por el que no se han incluido variables cuantitativas. ¿Habría alguna forma de incluirlas para enriquecer el análisis y las reglas de asociación? 

###Respuesta 7
Para realizar el algoritmo apriori es necesario que los campos sean categorías y no valores numéricos (pueden ser numéricos si estos representan categorías). Para que los campos numéricos representaran categorías bastaría con discretizar el dominio. Por ejemplo, en el caso de edad, bastaría con dividir en el dominio en (por ejemplo): 18-30, 30-40, >40. Se podría utilizar los datos observados en el k-mean como ayuda para elegir la discretización apropiada. 
```{r,eval=TRUE,echo=TRUE}
# Código respuesta 7
```

###Ejercicio 8

En el apartado 7.1 no se ha fijado la semilla antes de realizar el algoritmo apriori. ¿Cuál es el motivo de no incluirla?. Si se ejecuta el algoritmo varias veces, ¿Se obtendrán los mismos resultados? ¿Qué resultados cambian?. Justifique su respuesta y acompáñela de los ejemplos necesarios.

###Respuesta 8
En algoritmo apriori no tiene ninguna componente aleatoria, y por este motivo no es necesario fijar ninguna semilla. Obsérvese en el siguiente ejemplo como fijando dos semillas diferentes, los resultados son idénticos.

```{r,eval=TRUE,echo=TRUE}
# Código respuesta 8
seeds = c(123,321)
for (seed in seeds){
  print(paste("Seed fixed in ",seed, sep = " "))
  set.seed(seed)
  apriori(aso_car)
  inspect(apriori(aso_car))
}

```

###Ejercicio 9

¿Están relacionadas las reglas de asociación X->Y e Y->X?. Analice las relaciones entre el soporte y la esperanza de ambas reglas. Para facilitar la comprensión puede comparar la regla 10 del apartado 7.2 con la regla 5 del apartado 7.3.

Por definición el soporte es el mismo. Esto es así porque el soporte es la proporción de veces que se da la regla X->Y respecto del juego de datos. El número de veces que aparece la regla X->Y es el mismo al que se da la regla Y->X, puesto que es lo mismo, con lo cual el soporte es el mismo. Con respect a la confianza es diferente, ya que la confianza tiene en cuenta la proporción respecto del antecedente. Esto significa que en la regla X->Y, la confianza se calcula dividiendo el soporte de la regla entre la proporcion de veces que aparece el antecedente, en este caso X. En la regla Y -> X el antecedente es Y.


```{r,eval=TRUE,echo=TRUE}
# Código respuesta 9
```

###Ejercicio 10

En el apartado 7.3 se han fijado las consecuencias, pero es probable que en algunas circunstancias sólo conozcamos algunos antecedentes. Por ejemplo que sólo se disponga de la información sobre el sexo.
En este apartado se solicita calcular las reglas que se podrían extraer si sólo se conociese el sexo del cliente. Para ello cambie los parámetros por la siguiente expresión:
parameter = list ( supp = 0.05 , conf= 0.6 , target = "rules"),appearance = list(lhs=c("Sexo=Mujer","Sexo=Hombre"), default="rhs")

¿Cuántas reglas obtenemos? 

Interprete las reglas obtenidas y valore su eficacia.

###Respuesta 10
Las dos primeras reglas no tienen antecedente, con lo cual simplemente muestra una proporcion. La regla 3 y 4 tienen bajo soporte. Esto puede estar condicionado por la posibilidad de que haya menor número de mujeres que son clientes. La confianza es del 0.65 y el lift es menor 1. Es indica que no son reglas eficientes. Las reglas 5 y 6 son quizá mas interesantes porque tienen mayor soporte y especialmente la 6, tiene una lift mayor una confianza bastante mayor, casi del 0.8. Esta última regla podría sugerir que se podría realizar una campaña orientada a la captación de clientes de sexo masculino para que contraten dicho servicio. 

```{r,eval=TRUE,echo=TRUE}
apriori(aso_car,parameter = list ( supp = 0.05 , conf= 0.6 , target = "rules"),appearance = list(lhs=c("Sexo=Mujer","Sexo=Hombre"), default="rhs"))
inspect(apriori(aso_car,parameter = list ( supp = 0.05 , conf= 0.6 , target = "rules"),appearance = list(lhs=c("Sexo=Mujer","Sexo=Hombre"), default="rhs")))
```
